---

layout: single  
title: "容器网络与Service"  
date: 2022-05-29 19:19:00 +0800   
categories: k8s

---

# 容器网络

* “网络栈”，就包括了：网卡（Network Interface）、回环设备（Loopback Device）、路由表（Routing Table）和 iptables 规则。
* 每个Pod都各自的网络命名空间，故可以将pod想象成主机，多个pod的网络通信就是多个主机的通信。多个主机的通信需要交换机。而Linux中可以充当虚拟交换机的就是网桥（主要功能是根据 MAC 地址学习来将数据包转发到网桥的不同端口（Port）上。）。
* Docker 项目会默认在宿主机上创建一个名叫 docker0 的网桥，凡是连接在 docker0 网桥上的容器，就可以通过它来进行通信。
* Veth Pair 设备的特点是：它被创建出来后，总是以两张虚拟网卡（Veth Peer）的形式成对出现的。并且，从其中一个“网卡”发出的数据包，可以直接出现在与它对应的另一张“网卡”上，哪怕这两个“网卡”在不同的 Network Namespace 里。
* 容器网络通信流程:
	
	![容器网络通信流程](/assets/img/容器网络通信流程.jpg)
	
	注意：单机上的容器与容器间的通信，容器与宿主机的通信都是通过docker0网卡。
* 容器与其他宿主机通信:
	  
	![容器与其他宿主机通信](/assets/img/容器与其他宿主机通信.jpg)

## 容器跨主机网络

* Flannel 的三种容器跨主机网络的后端实现：
	* VXLAN：
	* host-gw：
	* UDP：性能最差

* 基于Flannel UDP模式的跨主通信的基本原理:
	
	![基于Flannel UDP模式的跨主通信的基本原理](/assets/img/基于FlannelUDP模式的跨主通信的基本原理.jpg) 
	
	详解：
	
	* flannel0 设备是一个TUN 设备，负责在**操作系统内核和用户应用程序之间传递 IP 包**。
	* flannel0匹配一个Flanneld 进程。	
	* flannel0分配的一个“子网”，容器的ip都属于这个子网。
	* Flanneld进程就是通过子网与宿主机ip的映射关系获得另外宿主机的ip地址。
	* docker0 网桥的地址范围必须是 Flannel 为宿主机分配的子网。

* 基于Flannel VXLAN模式的跨主通信的基本原理：
	
	![基于FlannelVXLAN模式的跨主通信的基本原理](/assets/img/基于FlannelVXLAN模式的跨主通信的基本原理.jpg) 
	
	详解： VXLAN使得连接在这个 VXLAN 二层网络上的“主机”（虚拟机或者容器都可以）之间，可以像在同一个局域网（LAN）里那样自由通信。而局域网中各个主机的发现是通过flannel.1 设备扮演的“网桥”角色。
	
## Kubernetes 网络模型与 CNI 网络插件

* Kubernetes 是通过一个叫作 CNI 的接口，维护了一个单独的网桥来代替 docker0。这个网桥的名字就叫作：CNI 网桥，它在宿主机上的设备名称默认是：cni0。
* Kubernetes 在启动 Infra 容器之后，就可以直接调用 CNI 网络插件，为这个 Infra 容器的 Network Namespace，配置符合预期的**网络栈**。
* **Flannel、Weave等属于容器跨主机网络方案，而CNI插件则属于此方案中pod中网络栈的配置的实现工具。**
* 实现一个给 Kubernetes 用的容器网络方案，其实需要做两部分工作：
	* 实现这个网络方案本身，比如Flannel ，在宿主机上安装 flanneld。
	* 实现该网络方案对应的 CNI 插件，比如Flannel方案中的flannel 和 portmap 这两个插件 。
	* 插件的调用：
		* 把一组CNI环境变量作为参数传给插件，CNI 配置文件里加载到的、默认插件的配置信息也传给插件，让插件把容器以 Veth Pair 的方式“插”到 CNI 网桥上；
		* 插件会在宿主机上检查 CNI 网桥是否存在，不存在，就创建
* Flannel 的 host-gw 模式和 Calico 项目
	* 待补充 

# Service

* Service 服务的主要作用，就是作为 Pod 的代理入口（Portal），从而代替 Pod 对外暴露一个固定的网络地址。
* 获取处于 Running 状态，且 readinessProbe 检查通过的 Pod：`kubectl get endpoints service名称`
* Service 是由 kube-proxy 组件，IPVS 模块，加上 iptables 来共同实现的。
* Service分为Normal Service和Headless Service。
* Normal Service：分配一个Virtual IP，即：虚拟 IP，提供的是 Round Robin 方式的负载均衡，匹配到的Pod后，会将流入的IP包的目的地址和端口改为此Pod 的 IP 地址和端口。
* Service 的 VIP只是一条 iptables 规则上的配置，并没有真正的网络设备，所以ping VIP，是不会有任何响应的。
* Headless Service不会分配VIP。
* 访问Service有两种访问方式：
	* 通过VIP：只能访问Normal Service。
	* 通过DNS 方式解析服务 ，比如：my-svc.my-namespace.svc.cluster.local。
		* 若为Normal Service：则DNS解析为VIP
		* 若为Headless Service：则DNS解析为my-svc所有被代理的 Pod 的 IP 地址的集合。如果客户端没办法解析这个集合的话，可能会只会拿到第一个 Pod 的 IP 地址。 
* Kubernetes 项目为代理的Pod 分配的唯一的“可解析身份”（Resolvable Identity）的DNS记录：
	* Normal Service：`<pod-name>.<namespace>.pod.cluster.local`
	* Headless Service：`<pod-name>.<svc-name>.<namespace>.svc.cluster.local` ，若Pod 本身声明了 hostname 和 subdomain 字段，则为`<hostname>.<subdomain>.<namespace>.svc.cluster.local`
* Service 分配公有 IP 地址：字段externalIPs，Kubernetes 要求 externalIPs 必须是至少能够路由到一个 Kubernetes 的节点。 

## Kubernetes 集群之外访问到 Kubernetes 里创建的 Service 

### NodePort

* 端口的范围默认是 30000-32767
* 在 NodePort 方式下，Kubernetes 会在 IP 包离开宿主机发往目的 Pod 时，对这个 IP 包做一次 SNAT 操作：将这个 IP 包的源地址替换成了这台宿主机上的 CNI 网桥地址，或者宿主机本身的 IP 地址（如果 CNI 网桥不存在的话）。
* 将 Service 的 spec.externalTrafficPolicy 字段设置为 local的作用：一台宿主机上的 iptables 规则，会设置为只将 IP 包转发给运行在这台宿主机上的 Pod。

### LoadBalancer

* 在公有云提供的 Kubernetes 服务里，都使用了一个叫作 CloudProvider 的转接层，来跟公有云本身的 API 进行对接。所以，在上述 LoadBalancer 类型的 Service 被提交后，Kubernetes 就会调用 CloudProvider 在公有云上为你创建一个负载均衡服务，并且把被代理的 Pod 的 IP 地址配置给负载均衡服务做后端。

### ExternalName

* 字段externalName设置域名，并且不需要指定 selector字段。当你通过 Service 的 DNS 名字访问它的时候，比如访问：my-service.default.svc.cluster.local。那么，Kubernetes 为你返回的就是字段externalName设置的域名。

# Ingress

* ingress服务：全局的、为了代理不同后端 Service 而设置的负载均衡服务，即service的service。

* ingress示例

	```
	apiVersion: extensions/v1beta1
	kind: Ingress
	metadata:
	  name: cafe-ingress
	spec:
	  tls:
	  - hosts:
	    - cafe.example.com
	    secretName: cafe-secret
	  rules:
	  - host: cafe.example.com
	    http:
	      paths:
	      - path: /tea
	        backend:
	          serviceName: tea-svc
	          servicePort: 80
	      - path: /coffee
	        backend:
	          serviceName: coffee-svc
          servicePort: 80
	```
		
* 字段解析：
	* rules 字段：在 Kubernetes 里，这个字段叫作：IngressRule。
	* host字段：必须是一个标准的域名格式（Fully Qualified Domain Name）的字符串，而不能是 IP 地址。
	* secretName：访问服务的证书和密钥：secret对象。
* 一个 Ingress 对象的主要内容，实际上就是一个“反向代理”服务（比如：Nginx）的配置文件的描述。而这个代理服务对应的转发规则，就是 IngressRule。
* 安装部署某一个具体Ingress Controller，Ingress Controller 会根据你定义的 Ingress 对象，提供对应的代理能力。
* 反向代理项目，比如 Nginx、HAProxy、Envoy、Traefik 等，都已经为 Kubernetes 专门维护了对应的 Ingress Controller。
* 当一个新的 Ingress 对象由用户创建后，nginx-ingress-controller 就会根据 Ingress 对象里定义的内容，生成一份对应的 Nginx 配置文件（/etc/nginx/nginx.conf），并使用这个配置文件启动一个 Nginx 服务。
* 如果这里只是被代理的 Service 对象被更新，nginx-ingress-controller 所管理的 Nginx 服务是不需要重新加载（reload）的。这当然是因为 nginx-ingress-controller 通过Nginx Lua方案实现了 Nginx Upstream 的动态配置。
* nginx-ingress-controller 还允许你通过 Kubernetes 的 ConfigMap 对象来对上述 Nginx 配置文件进行定制。这个 ConfigMap 的名字，需要以参数的方式传递给 nginx-ingress-controller。而你在这个 ConfigMap 里添加的字段，将会被合并到最后生成的 Nginx 配置文件当中。
* Ingress Controller 也允许你通过 Pod 启动命令里的–default-backend-service 参数，设置一条默认规则，比如：–default-backend-service=nginx-default-backend。这样，任何匹配失败的请求，就都会被转发到这个名叫 nginx-default-backend 的 Service。所以，你就可以通过部署一个专门的 Pod，来为用户返回自定义的 404 页面了。
* Ingress 只能工作在七层，而 Service 只能工作在四层。所以当你想要在 Kubernetes 里为应用进行 TLS 配置等 HTTP 相关的操作时，都必须通过 Ingress 来进行。

	

